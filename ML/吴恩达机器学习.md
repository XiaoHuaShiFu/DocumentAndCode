# 2、单变量线性回归

1. 模型描述

   - 符号

     ![image-20200303225755999](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303225755999.png)

   - 单变量线性回归

     ![image-20200303230145257](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303230145257.png)

   - hypothesis函数就是从输入x得到输出y的函数

   - 模型参数

     ![image-20200303230326701](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303230326701.png)

     - 也就是θi

   - cost function

     ![image-20200303231244530](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231244530.png)

     - 也就是J，要求的是θ0和θ1使得J最小，这样和样本数据误差就最小了，单变量线性回归问题是一个求最小值问题。

   - 模型公式

     ![image-20200303231644304](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231644304.png)

   - h函数和J函数区别的一个例子

     ![image-20200303231919760](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231919760.png)

     - h函数是使用的函数，J函数是描述h函数的准确性，求的是让J函数最小时的参数θ0和θ1

     - 当只有一个参数θ时的J函数：很像二次函数

       ![image-20200303232424169](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303232424169.png)

     - 当两个参数时J的函数：也是一个碗形

       ![image-20200303232733769](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303232733769.png)

     - 使用等高图：同一条线上表示相同的J函数值，也就是上图碗形的同一高度的一圈

       ![image-20200303233233498](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303233233498.png)

       - 离中间那个点越近表示J值越小，也就是越符合数据集，也就是h函数越好

2. 梯度下降

   ![image-20200303233747940](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303233747940.png)

   - 如上图所示，也就是一点点的改变θ0和θ使得J尽可能小。

   - 梯度下降的数学表达：

     - ![image-20200303235001115](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303235001115.png)

     - 要同时更新θ0和θ1，也就是correct simultaneous update更新

     - 其中偏导项就是某个方向的增长率，α就是往那个方向的步长，就是一直往小的方向走。

       ![image-20200304000022073](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000022073.png)

     - 如上图，如果α太小，收敛很慢；如果α太大，可能找不到局部最优，甚至发散。

       ![image-20200304000353318](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000353318.png)

     - 由于梯度下降的特点：导数值会在接近局部最优点的时候慢慢变小，因此没必要减小α的值。

   - 偏导数项的求导结果：

     ![image-20200304000957638](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000957638.png)

   - 所以重复的过程变成如下：

     ![image-20200304001129202](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304001129202.png)

   - 线性回归的梯度下降只有一个局部最优，也就是全局最优，因此直接找局部最优就是全局最优：

     ![image-20200304012803400](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304012803400.png)

   - 线性回归的梯度下降也成为Batch Gradient Descent，也就是因为每次都要计算整个Batch的数据集。

     ![image-20200304013202089](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013202089.png)

3. 线性代数

   - 矩阵Matrix：默认用大写字母表示

     ![image-20200304013656753](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013656753.png)

   - 向量Vector：默认用小写字母表示

     ![image-20200304013939052](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013939052.png)

   - 矩阵和向量默认下标从1开始

   - 使用h函数和数据集X预测Y

     ![image-20200304014900134](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304014900134.png)

     - 也就是输入数据集X的矩阵*h函数的矩阵->结果Y矩阵

   - 使用多个h函数和数据集X预测多个Y：也就是第二个矩阵多了几列

     ![image-20200304015415312](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304015415312.png)

   - 矩形乘法不满足交换律，矩阵乘法满足结合律

   - 单位矩阵：乘了单位矩阵还是等于原矩阵

     ![image-20200304015853265](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304015853265.png)

   - 逆矩阵：和原矩阵相乘=单位矩阵，只有方阵才有逆矩阵

     ![image-20200304020401373](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304020401373.png)

     - 没有逆矩阵的矩阵称为奇异矩阵

   - 转置矩阵

     ![image-20200304020523994](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304020523994.png)

4. 多特征：也就是多个影响y的值x1，x2，。。。

   - 符号

     ![image-20200304203613348](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304203613348.png)

   - 多特征量的数学表达形式

     ![image-20200304204102356](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204102356.png)

   - 多特征量的表示和梯度下降表示：其中很多都用了向量去简化表示θ

     ![image-20200304204310332](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204310332.png)

   - 多元梯度下降的表示：和一元差不多，一元是多元的特例

     ![image-20200304204657822](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204657822.png)

   - 特征值只要接近[-1,1]这个范围就可以，不用一定都在这个范围：

     ![image-20200304205557925](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304205557925.png)

   - 特征缩放：不需要太准确，只是为了让梯度下降更快。这里有归一化和标准化两种

     ![image-20200304210043620](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304210043620.png)

   - 学习率的选择和收敛的判断

     - 收敛判断：1、看随着梯度下降次数代加函数的图像；2、看两次梯度下降的差是否小于一个很小的常数。

       ![image-20200304210529625](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304210529625.png)

     - 学习率选择：不断尝试。不能太小，也不能太大。

       ![image-20200304211111397](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211111397.png)

   - 如果多次方程可以更好拟合：通过一些技巧可以转换成线性的

     ![image-20200304211644251](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211644251.png)

     - 如上，把x^2、x^3转换成房子的面积和体积。然后这里需要归一化，因为相差较大。

     - 也可以用根号x去拟合特征：

       ![image-20200304211932230](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211932230.png)

   - 正规方程

     - 使得代价函数J最小的θ求法：

       ![image-20200304212656371](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304212656371.png)

     - Octave的公式写法

       ![image-20200304213254381](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304213254381.png)

     - 正规方程不需要特征缩放。

   - 正规方程和梯度下降的选择：小于10000可以选择正规方程，大于10000选择梯度下降。正规方程需要做O(n^3)的逆矩阵算法。

     ![image-20200304213752316](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304213752316.png)

   - 如果XTX不可逆的情况：①很少出现。②一般是有参数多余（比如x1=3.28x2），那么x1和x2其中有一个是多余的。或者比如特征100个，但是数据集只有10个。也会出现不可逆的情况。③使用pinv函数来计算即使是不可逆也会得到正确的结果。

     ![image-20200304235143720](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304235143720.png)

5. Octave入门

   - 加减乘除指数：+、-、*、/、^
   - 注释：%
   - 假：0
   - 等于、不等于：==、~=
   - 与、或、非、异或：&&、||、~、XOR(a, b)
   - 改变提示符：PS1('提示符')
   - 赋值：=
   - 显示：
     - 去掉ans=：disp(a)
     - 去掉输出：;
     - 格式化（类c和java）：sprintf('xxxx %d', 4)
     - 显示格式：format long %更多小数点后的位数、format short %更少小数点后的位数
     - 图像显示：
       - 直方图：hist(w)，hist(w, 50) %50是输出的条数
   - π：pi
   - 根号2：sqrt(10)
   - 随机数：rand()
   - 矩阵：[1 2; 3 4; 5 6] % 三行二列
     - 生成全1矩阵：ones(2,3) % 二行三列的全1矩阵
     - 生成全0矩阵：zeros(1,3)
     - 生成单位矩阵：eye(4)
     - 生成随机矩阵：rand(2,3)
     - 生成服从高斯分布的矩阵：randn(1,3)
     - 矩阵大小：size(A)
       - 行：size(A, 1)
       - 列：size(A, 2)
   - 向量便捷方法：1:0.1:1.5 % 从1到1.5每间隔0.1的数，也就是1.0 1.1 1.2 1.3 1.4 1.5
     - 方法2：1:6 % 从1到6每间隔1的数，也就是1 2 3 4 5 6
     - 向量长度：length(a)
   - 帮助命令：help rand
   - 显示变量命令：who
   - 当前路径：pwd
   - 当前文件夹内容：ls
   - 加载文件：load featuresX.dat 
     - 或者：load('featuresX.dat')