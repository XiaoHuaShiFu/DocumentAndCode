# 2、单变量线性回归

1. 模型描述

   - 符号

     ![image-20200303225755999](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303225755999.png)

   - 单变量线性回归

     ![image-20200303230145257](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303230145257.png)

   - hypothesis函数就是从输入x得到输出y的函数

   - 模型参数

     ![image-20200303230326701](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303230326701.png)

     - 也就是θi

   - cost function

     ![image-20200303231244530](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231244530.png)

     - 也就是J，要求的是θ0和θ1使得J最小，这样和样本数据误差就最小了，单变量线性回归问题是一个求最小值问题。

   - 模型公式

     ![image-20200303231644304](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231644304.png)

   - h函数和J函数区别的一个例子

     ![image-20200303231919760](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303231919760.png)

     - h函数是使用的函数，J函数是描述h函数的准确性，求的是让J函数最小时的参数θ0和θ1

     - 当只有一个参数θ时的J函数：很像二次函数

       ![image-20200303232424169](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303232424169.png)

     - 当两个参数时J的函数：也是一个碗形

       ![image-20200303232733769](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303232733769.png)

     - 使用等高图：同一条线上表示相同的J函数值，也就是上图碗形的同一高度的一圈

       ![image-20200303233233498](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303233233498.png)

       - 离中间那个点越近表示J值越小，也就是越符合数据集，也就是h函数越好

2. 梯度下降

   ![image-20200303233747940](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303233747940.png)

   - 如上图所示，也就是一点点的改变θ0和θ使得J尽可能小。

   - 梯度下降的数学表达：

     - ![image-20200303235001115](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200303235001115.png)

     - 要同时更新θ0和θ1，也就是correct simultaneous update更新

     - 其中偏导项就是某个方向的增长率，α就是往那个方向的步长，就是一直往小的方向走。

       ![image-20200304000022073](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000022073.png)

     - 如上图，如果α太小，收敛很慢；如果α太大，可能找不到局部最优，甚至发散。

       ![image-20200304000353318](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000353318.png)

     - 由于梯度下降的特点：导数值会在接近局部最优点的时候慢慢变小，因此没必要减小α的值。

   - 偏导数项的求导结果：

     ![image-20200304000957638](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304000957638.png)

   - 所以重复的过程变成如下：

     ![image-20200304001129202](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304001129202.png)

   - 线性回归的梯度下降只有一个局部最优，也就是全局最优，因此直接找局部最优就是全局最优：

     ![image-20200304012803400](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304012803400.png)

   - 线性回归的梯度下降也成为Batch Gradient Descent，也就是因为每次都要计算整个Batch的数据集。

     ![image-20200304013202089](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013202089.png)

3. 线性代数

   - 矩阵Matrix：默认用大写字母表示

     ![image-20200304013656753](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013656753.png)

   - 向量Vector：默认用小写字母表示

     ![image-20200304013939052](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304013939052.png)

   - 矩阵和向量默认下标从1开始

   - 使用h函数和数据集X预测Y

     ![image-20200304014900134](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304014900134.png)

     - 也就是输入数据集X的矩阵*h函数的矩阵->结果Y矩阵

   - 使用多个h函数和数据集X预测多个Y：也就是第二个矩阵多了几列

     ![image-20200304015415312](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304015415312.png)

   - 矩形乘法不满足交换律，矩阵乘法满足结合律

   - 单位矩阵：乘了单位矩阵还是等于原矩阵

     ![image-20200304015853265](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304015853265.png)

   - 逆矩阵：和原矩阵相乘=单位矩阵，只有方阵才有逆矩阵

     ![image-20200304020401373](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304020401373.png)

     - 没有逆矩阵的矩阵称为奇异矩阵

   - 转置矩阵

     ![image-20200304020523994](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304020523994.png)

4. 多特征：也就是多个影响y的值x1，x2，。。。

   - 符号

     ![image-20200304203613348](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304203613348.png)

   - 多特征量的数学表达形式

     ![image-20200304204102356](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204102356.png)

   - 多特征量的表示和梯度下降表示：其中很多都用了向量去简化表示θ

     ![image-20200304204310332](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204310332.png)

   - 多元梯度下降的表示：和一元差不多，一元是多元的特例

     ![image-20200304204657822](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304204657822.png)

   - 特征值只要接近[-1,1]这个范围就可以，不用一定都在这个范围：

     ![image-20200304205557925](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304205557925.png)

   - 特征缩放：不需要太准确，只是为了让梯度下降更快。这里有归一化和标准化两种

     ![image-20200304210043620](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304210043620.png)

   - 学习率的选择和收敛的判断

     - 收敛判断：1、看随着梯度下降次数代加函数的图像；2、看两次梯度下降的差是否小于一个很小的常数。

       ![image-20200304210529625](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304210529625.png)

     - 学习率选择：不断尝试。不能太小，也不能太大。

       ![image-20200304211111397](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211111397.png)

   - 如果多次方程可以更好拟合：通过一些技巧可以转换成线性的

     ![image-20200304211644251](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211644251.png)

     - 如上，把x^2、x^3转换成房子的面积和体积。然后这里需要归一化，因为相差较大。

     - 也可以用根号x去拟合特征：

       ![image-20200304211932230](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304211932230.png)

   - 正规方程

     - 使得代价函数J最小的θ求法：

       ![image-20200304212656371](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304212656371.png)

     - Octave的公式写法

       ![image-20200304213254381](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304213254381.png)

     - 正规方程不需要特征缩放。

   - 正规方程和梯度下降的选择：小于10000可以选择正规方程，大于10000选择梯度下降。正规方程需要做O(n^3)的逆矩阵算法。

     ![image-20200304213752316](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304213752316.png)

   - 如果XTX不可逆的情况：①很少出现。②一般是有参数多余（比如x1=3.28x2），那么x1和x2其中有一个是多余的。或者比如特征100个，但是数据集只有10个。也会出现不可逆的情况。③使用pinv函数来计算即使是不可逆也会得到正确的结果。

     ![image-20200304235143720](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200304235143720.png)

5. Octave入门

   - 加减乘除指数：+、-、*、/、^

   - 注释：%

   - 假：0

   - 等于、不等于：==、~=

   - 与、或、非、异或：&&、||、~、XOR(a, b)

   - 赋值：=

   - 根号2：sqrt(10)

   - 对数：log(a)

   - 以e为底的指数：exp(v) %可以是矩阵

   - 绝对值：abs(v)

   - 最大值、最小值：max(v); min(v) % 可用于矩阵

     - [val, idx] = max(v) % 可以返回对应的下标

   - 寻找满足条件的：find(v < 3) % 返回v小于3的元素的下标

     - [r, idxc= find(v<3) % 可以返回对应的行列下标

   - 求和、求积：sum(v); prod(v)

   - 向下取整、向上取整、四舍五入：floor(a); ceil(a); round(a)

   - sin、cos、tan: sin(a); cos(a); tan(a)

   - 求逆：pinv(3)

   - 随机数：rand()

   - π：pi

   - 

   - 矩阵：[1 2; 3 4; 5 6] % 三行二列

     - 生成全1矩阵：ones(2,3) % 二行三列的全1矩阵
     - 生成全0矩阵：zeros(1,3)
     - 生成单位矩阵：eye(4)
     - 生成随机矩阵：rand(2,3)
     - 生成服从高斯分布的矩阵：randn(1,3)
     - 矩阵大小：size(A)
       - 行：size(A, 1)
       - 列：size(A, 2)
     - 矩阵访问：A(3,2) % 同数组，只是下标从1开始
       - 访问行：A(3,:) 
       - 访问列：A(:,2)
       - 某几行：A([1, 3], :)
       - 某几列：A(:, [1, 2])
       - 替换行：A(1, :) = [1, 2]
       - 替换列：A(:, 1) = [1; 2; 3]
       - 增加行：A = [A; [3, 4]]
       - 增加列：A = [A, [3; 4; 5]]
       - 转成列向量：A(:)
       - 矩阵拼接：
         - 行拼接：[A, B]
         - 列拼接：[A; B]
     - 矩阵乘法：A * B
     - 相对于元素相乘：A .* B
       - 相除：A./B
     - 每个元素扩展n倍：A .* 2
       - 缩小：A./2
     - 转置矩阵：A'
     - 向量复制向量长度次数：meshgrid(x,y)

   - 向量便捷方法：1:0.1:1.5 % 从1到1.5每间隔0.1的数，也就是1.0 1.1 1.2 1.3 1.4 1.5

     - 方法2：1:6 % 从1到6每间隔1的数，也就是1 2 3 4 5 6
     - 向量长度：length(a)
     - 一个矩阵的第一列的某些行：A(1:10) % 第1列的1-10行

   - 帮助命令：help rand

   - 显示变量命令：who，whos

   - 删除变量命令：clear  v %删除某个变量

     - 删除所有变量：clear

   - 当前路径：pwd

   - 当前文件夹内容：ls

   - 加载文件：load featuresX.dat 

     - 或者：load('featuresX.dat')

   - 保存文件： save filename.mat v % 文件名和变量名

     - 用文本形式保存：save hello.txt v -ascii % ascii是指定编码

   - 增加路径：addpath('C:\\Users\\xhsf\\Desktop')

   - 改变提示符：PS1('提示符')

   - 显示：

     - 去掉ans=：disp(a)
     - 去掉输出：;
     - 格式化（类c和java）：sprintf('xxxx %d', 4)
     - 显示格式：format long %更多小数点后的位数、format short %更少小数点后的位数
     - 图像显示：
       - 直方图：hist(w)，hist(w, 50) %50是输出的条数
       - 曲线图：plot(w)
       - 保持原图：hold on
       - 设置label：xlabel('ttt'); ylabel('xxx')
       - 设置legend:legend('xxx', 'xxxx')
       - 设置title: title('xxxzzz')
       - 保存图片：print -dpng 'test.png'
       - 关闭图像：close
       - 指定图像序号：figure(2) % 用于同时打开多个图像
       - 划分图像：subplot(1, 2, 1) % 将划分成1行2列，选中第一个
       - 设置坐标：axis([0.5, 1, -1, 3])
       - 画直线：ezplot(‘y-k*x-b’)、plot ([0,5],[3,3])
       - 清空图像：clf
       - 画三维图：[xx, yy, zz] = meshgrid(x,y,z); surf(xx,yy,zz)
       - ![image-20200306210523682](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200306210523682.png)

   - 控制指令

     - for:

       ```octave
       for i = 1 : 10
       	a(i) = 2 ^ i
       end
       ```

     - while:

       ```octave
       while i <= 5
       	s(i) = i ^ 2
       	i++
       end
       ```

     - if

       ```octave
       if i == 6 
       	s
       elseif i == 7
       	a
       else 
       	v
       end
       ```

     - break和continue可以用

     - 定义函数：

       ```octave
       function y = squreThisNumber(x)
       y = x ^ 3
       end;
       ```

     - 多个返回值函数：

       ```
       function [a, b] = squreThisNumber(x)
       a = x ^ 2
       b = x ^ 3
       end;
       ```

       

6. 分类

   1. ​	Logistic Regression（用于y位0or1的分类问题）

      ![image-20200307142517230](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307142517230.png)

   2. 将假设值转换到0~1之间

      ![image-20200307143323115](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307143323115.png)

   3. h函数是想表达1的概率是多少（或0的概率是多少）

      ![image-20200307143821395](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307143821395.png)

   4. 预测y是0或者是1的判断条件：也就是看z即θTx的大小。其实θTx是一条线，这条线把图像分为0的部分和1的部分，大于等于和小于是分为两个部分的判断条件。就是像坐标轴上，一条直线通过大于等于0和小于0把坐标轴分位两部分。

      ![image-20200307145956184](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307145956184.png)

   5. 决策边界：也就是4所说的东西。假设函数就是那个决策边界（当然通过	Logistic Function）。hx=0.5就是那条线（也可以说，hx=0.5时，那些点就在那条线上）。

      ![image-20200307150926255](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307150926255.png)

      - 圆的决策边界:

        ![image-20200307151814824](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307151814824.png)

      - 或者更复杂的：。。

        ![image-20200307151859272](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307151859272.png)

   6. 代价函数

      - 原sigmoid是一个非凸函数，要找一个凸函数

      - log代价函数：也就是y=1的时候，如果h也接近1，那么代价就小。y==0的时候h接近0，代价也小。反之就大。

        ![image-20200307153355529](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307153355529.png)

      - logistic regression cost function

        ![image-20200307153747509](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307153747509.png)

      - 目标：最小化J

        ![image-20200307154039498](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307154039498.png)

   7. logistic regression的梯度下降

      ![image-20200307154550260](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307154550260.png)

   8. 优化算法

      - 三种优化算法：不需要手动选择α，更快的梯度下降。可以直接找库函数 用。

        ![image-20200307155009034](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307155009034.png)

      - 在octave里的使用优化算法的方法：也就是只需要写一个代价函数costFunction

        ![image-20200307155750031](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307155750031.png)

   9. 多分类算法Multiclass classification

      - 划分成三个h

        ![image-20200307160511824](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307160511824.png)

      - 然后在输入x时，选择h最高的那个y作为输出

        ![image-20200307160702542](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307160702542.png)

7. 过度拟合

   1. 过度拟合：如图3

      ![image-20200307172054893](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307172054893.png)

      ![image-20200307172227763](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307172227763.png)

      - 减少过度拟合的方法：

        ![image-20200307172507016](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307172507016.png)

   2. 正则化：加入正则化参数，来减小某些项对代价函数的影响，λ是正则化参数

      ![image-20200307173219568](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307173219568.png)

   3. 加入正则化的梯度下降公式：其实也就是在θj加入一个参数，这个参数接近1，小于1，这样可以使得原θ缩小一点，也就是原θ影响小一点。

      - 个人理解：就是要使得这一项影响小一点，就缩小这一项。

      ![image-20200307173820097](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307173820097.png)

      - 使用正则化时的正规方程：

        ![image-20200307202108075](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307202108075.png)

   4. logistic regression的梯度下降的正则化

      - cost function：

        ![image-20200307202614395](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307202614395.png)

      - gradient descent：

        ![image-20200307202647472](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307202647472.png)

      - 使用优化库时：只需要输入添加了正则项的cost function

        ![image-20200307203016315](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307203016315.png)

8. neutral networks

   1. 模型：用每个点的灰度（如RGB）作为一维。特征量会很大。

      ![image-20200307215946592](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307215946592.png)

   2. neuron model：Logistic unit，单个神经元，也就是Logistic regression那个例子。

      ![image-20200307221020534](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307221020534.png)

      - neural network：很多的神经单元

        ![image-20200307221239724](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307221239724.png)

      - neural network definition：重点是Θ，这个是权重矩阵，也是参数，它是从上一层到下一层时g公式里的参数。通过改变Θ得到不同的输出。

        ![image-20200307221925431](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307221925431.png)

   3. neural network 计算过程：

      - forward propagation：向前传播。计算一个激活项a，然后用这个a和Θ去计算下一个激活项。第一个激活项a1=x，也就是输入的参数。第二层需要加一个bias unit，也就是1，常数项。

        ![image-20200307223156136](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307223156136.png)

      - 很像逻辑回归，但是不是用x1、x2。。作为特征，而是用a1、a2。。作为特征。

   4. 例子

      1. AND计算

         ![image-20200307225027847](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307225027847.png)

      2. XNOR计算：组合三种运算（也就是3个神经元）

         ![image-20200307233752153](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200307233752153.png)

      3. 多元分类

         - 模型：也就是目标y是一个向量。所以h也是一个向量。

           ![image-20200308000840739](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308000840739.png)

9. neural network实现

   1. 代价函数

      - 符号：L是层数，sl是l层的单元数

        ![image-20200308001617837](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308001617837.png)

      - 代价函数：类似于上面的代价函数，只是这里更加通用了。这里的K=1时就是之前的函数。只是这里考虑了h是一个向量的情况。然后也考虑了多层的特征值情况。

        ![image-20200308002808552](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308002808552.png)
      
        ![image-20200308141811198](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308141811198.png)
   
   2. 反向传播
   
      - 向前传播
   
        ![image-20200308142657149](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308142657149.png)
   
      - 反向传播：把后面一层的误差δ传播到前面一层。这样如果后一层误差大，那么前面一层也会改变得大一点。应该是这个意思。
   
        ![image-20200308144248256](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308144248256.png)
   
      - 反向传播的计算过程：这里同上，只是有m的输入的数据，进行累加求均值，然后加了正则化项而已。
   
        ![image-20200308145540815](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308145540815.png)
   
      - δ其实就是代价在某个方向的导数，每个Θ就是一个方向，其实也就是类似于改变某个方向的Θ，通过增长率（导数），直到导数趋近于0。也就把代价进行反向传播。也就是改变梯度项Θ的速度。而Θ改变又影响J，当Θ趋向于0时（也就是Θ不变了），J也最小。
   
   3. 使用优化算法
   
      - 需要把theta矩阵展开成向量：
   
        ![image-20200308153536872](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308153536872.png)
   
      - 展开方法：
   
        ![image-20200308155224719](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308155224719.png)
   
      - 使用算法方法：得到初始化参数，展开，计算costFunction（合并Θ，计算D和J，展开D）
   
        ![image-20200308160104897](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308160104897.png)
   
   4. 梯度检测
   
      - 用每个方向Θ的导数去和D比较。其实D就是每个Θ方向上的斜率（导数）。
   
        ![image-20200308161449677](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308161449677.png)
   
      - 梯度检测的步骤和注意点：其实这里也验证了，反向传播其实就是另外一个求导的方法而已
   
        ![image-20200308162012404](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308162012404.png)
   
   5. 随机初始化：就是初始化theta要随机取接近0的值，不要取都是0。
   
      ![image-20200308165259755](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308165259755.png)
   
   6. 总结
   
      - 网络结构和输入输出：结构一般是单隐藏层，如果多隐藏层，每一层的单元数一般是一样的。输入是x，输出一般是01的向量。
   
        ![image-20200308165751781](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308165751781.png)
   
      - 计算过程：先求h，然后求J，然后求D（也就是J导数，用反向传播算法），然后验证（可以删除），然后使用优化算法求theta。
   
        ![image-20200308170406176](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308170406176.png)
   
        ![image-20200308170647060](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308170647060.png)
   
        - 注：神经网络是一个凸函数，因此可能算出来的不是全局最优，但是一般这些算法算出来的结果都很优了。 
   
10.  

    1. 决定下一步做什么

       - 一般可以优化算法的方法：但是有可能会没用

         ![image-20200308211043153](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308211043153.png)

       - Machine learning diagnostic：判断某种方式是否可以优化算法

         ![image-20200308211130281](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308211130281.png)

    2. 评估hypothesis

       - 分割数据集：73原则，7成去训练，3成去测试

         ![image-20200308211456710](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308211456710.png)

       - 测试误差：一种是花费，一种是是不是猜测对（只在逻辑回归有用）

         ![image-20200308211704974](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308211704974.png)

         ![image-20200308211922682](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308211922682.png)

    3.    模型选择、训练、验证和测试集

       - 模型选择：使用多个模型，然后进行训练，用测试集算出Jtest，选择Jtest最小的。

         ![image-20200308212747817](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308212747817.png)

       - 分成3个子集的方式：训练；用cv选最小Tcv；用test检验所选的模型是否正确。

         ![image-20200308213211734](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308213211734.png)

         ![image-20200308213301503](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308213301503.png)

    4. 偏差和方差的判断

       ![image-20200308222102858](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308222102858.png)

    5. 正则化、偏差和方差

       - 过大和过小的λ会导致偏差和方差

         ![image-20200308222432851](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308222432851.png)

       - 训练误差随着λ增大而增大，测试误差是一个二次曲线，中间某点是最低。也就是λ太大，那么数据就会拟合得不好，太小就过度拟合。拟合得不好测试的误差也大，拟合得太好测试误差也大。

         ![image-20200308223142465](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308223142465.png)

    6. 学习曲线

       - 较好的情况：随着训练集增大，Jcv慢慢降低。

         ![image-20200308223901016](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308223901016.png)

       - 高偏差的情况：很高的Jcv和Jtran，且很快变成直线，不再怎么变化。因此加大数据集不能改善算法。

         ![image-20200308224127906](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308224127906.png)

       - 高方差：很低的Jtran和很高的Jcv，随着数据集的增大，两个曲线会接近，所以增大数据集有用。

         ![image-20200308224514894](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308224514894.png)

    7. 选择改良算法的方法：

       ![image-20200308225221369](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308225221369.png)

    8. neural network and overfitting：大型网络一般性能更好，但是容易出现过拟合现像（通过正则化修复），而且计算量大。

       ![image-20200308225516998](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200308225516998.png)

11.  

    1. 确定执行优先级

       - 垃圾邮件分类器：使用垃圾邮件中常出现的关键词作为特征。然后考虑一些优化算法，比如查看发送人，标题，和错误拼写单词。

         ![image-20200309210300849](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200309210300849.png)

         ![image-20200309210530513](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200309210530513.png)

    2. 误差分析 

       - 实现一个模型的方法：简单模型进行测试，用交叉数据检验，然后误差分析。

         ![image-20200309211026724](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200309211026724.png)

       - 通过实际运行模型，然后交叉检验来判断模型是否得到优化：

         ![image-20200309211843073](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200309211843073.png)

    3. 不对称性分类的误差评估

       - 也就是比如100个只有1个y是1，其他都是0。这种情况下，如果直接返回y=0，准确率将高达99%。但是这个算法并不好。通过下面的两个方法：precision和recall可以避免这种算法的误判。这两个值都是越高越好。

         ![image-20200309214421207](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200309214421207.png)

    4. pricision and recall 的权衡

       - 如果临界值大，那么precision会高，recall会低。也就是临界值大，预测的1会更准，所以和真实情况也更接近。而临界值越高，预测也就更谨慎，因此true positives会低一点，因此recall也会低。

       - 临界值和召回率的关系

         ![image-20200310152103675](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310152103675.png)

       - 使用Fscore评估算法的临界值和召回率是否好：

         ![image-20200310152535229](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310152535229.png)

    5. 机器学习数据：如果特征少，可能大量数据作用不大。但是如果特征多，那么大量数据会得到更好的算法。

       ![image-20200310162552371](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310162552371.png)

12.  支持向量机

    1. 优化目标

       - logistic regression

         ![image-20200310163427010](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310163427010.png)

       - alternative view of logistic regression：其实也就是通过两个直线去表示代价函数的曲线。

         ![image-20200310163540361](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310163540361.png)

       - SVM hypothesis：其实也就是去掉1/m，然后把λ换成C移到前面，然后把h函数换成更简单的形式的logistic regression。减少计算量。

         ![image-20200310164235987](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310164235987.png)

       - SVM的cost function图像：也就是有一个阈值，当大于这个阈值，cost=0。小于某个阈值，cost=0。

         ![image-20200310164549047](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310164549047.png)

    2. large margin classifier：SVM will lead margin large，e.g. black line

       ![image-20200310165156416](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310165156416.png)

       - 当C太大时，可能会是紫线，也就是对异常数据点敏感。当C小时，会是黑线，尽量加大间距。

         ![image-20200310165522240](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310165522240.png)

    3. large margin classifier math 原理：这里是转换成内积的概念p||θ||

       ![image-20200310172226710](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310172226710.png)

       - 其实也就是large margin会扩大p，因此||θ||小，因此cost小。

         ![image-20200310173142917](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310173142917.png)

       

    4. 核函数

       - 定义：也就是用k函数去表达一个f函数，f函数可以是x1，x2，x1x2，x1^2等。

         ![image-20200310174149256](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310174149256.png)

       - 这也是因为，如果x和l距离很近，那么f≈1。如果很远，f≈0。

         ![image-20200310174607873](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310174607873.png)

       - 核函数的图像：在顶点就是x和l重叠的地方。σ也会影响下降速度，越大下降越慢。

         ![image-20200310225321972](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310225321972.png)

       - 核函数分类的原理：距离l近的x，f函数的值接近于1，远离的，接近0。因此远离的f项会被忽略，因此进行分类（在同一个l附近的x会拥有接近的计算结果）。

         ![image-20200310230050844](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310230050844.png)

    5. 核函数

       - SVM with kernels：把x用核函数转换成f

         ![image-20200310230930451](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310230930451.png)

       - SVM with kernels 目标：最小化代价函数，这里的正则项用另外一个近似的代替，加快计算速度。

         ![image-20200310231559389](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310231559389.png)

       - 参数C和σ^2的选择：

         ![image-20200310231903424](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310231903424.png)

    6. 使用SVM

       - 使用库函数实现：有liner kernel和gaussian kernel两种，liner kernel不需要核函数，gaussian kernel 需要，还要决定σ。

         ![image-20200310232410664](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310232410664.png)

       - 进行核函数计算前需要对数据进行缩放处理：

         ![image-20200310232831827](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310232831827.png)

       - 其他的核函数：用于特定的应用

         ![image-20200310233215614](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310233215614.png)

       - 多分类的SVM：也是使用K分类

         ![image-20200310233347414](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310233347414.png)

       - logistic regression and SVMs and Neural network choose：大n小m用logistic regression or SVM without a kernel；小n相对大m用SVM withe Gaussian kernel；小n非常大m增加特征并使用logistic regression or SVM without a kernel，因为这样快一点。Neural network 速度比较慢，但是比较准。

         ![image-20200310233929939](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200310233929939.png)
    
13. unsupervised learning

    1. unsupervised learning introduction

    2. K-Means

       - 随机选K个中心，重复（分配点到最近的中心，改变中心的位置）

         ![image-20200313005111292](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313005111292.png)

       - K-means for non-separated clusters：还是会有效，如右图

         ![image-20200313005329491](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313005329491.png)

    3. K-means optimization objective：也就是平均距离最小。其实迭代过程也就是最小化J的过程。

       ![image-20200313011208954](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313011208954.png)

       ![image-20200313011412393](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313011412393.png)

    4. random initialization

       - 选取K个点作为中心

         ![image-20200313011912032](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313011912032.png)

       - local optima：避免local optima的方法是多次random initialization然后运行K-means算法，然后选取最小J的那一次。通常在K较小（2-10）的时候很有效。但K太大效果不明显。

         ![image-20200313012258605](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313012258605.png)

    5. K choose

       - Elbow method：有时间有用（左），有时候没用（右）

         ![image-20200313012738291](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313012738291.png)

14. Dimensionality Reduction

    - 数据压缩：真的就是降维
    - 可视化：也就是把维度投影到更低的维度。如2维，这样就可以画图了。

    1. Principal Component Analysis (PCA) problem formulation

       - 原理：点到低维平面的距离最短。

         ![image-20200313152854528](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313152854528.png)

       - PCA is not linera regression：一个是y方向的平方差，另外一个是与直线的平方差。

         ![image-20200313153118900](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313153118900.png)

    2. Principal Component Analysis algorithm

       - 数据缩放和均值正规化

         ![image-20200313153938853](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313153938853.png)

         - 计算covariance matrix，然后计算eigenvectors of matrix Σ，然后取U矩阵的前k列即可。

           ![image-20200313155240958](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313155240958.png)

         - 然后把前k列逆置然后乘以X，即可得到k×1的向量z

           ![image-20200313155559341](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313155559341.png)

         - octave实现过程：

           ![image-20200313155824810](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313155824810.png)

    3. Choosing k ，number of principal components

       - 保留 variance，同时减少维度k，一般是保留99% variance。

         ![image-20200313161348410](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313161348410.png)

       - 只需要使用svd计算的S，然后选取k从1到n，然后找到第一个小于0.01的即可，不需要重复计算

         ![image-20200313161832628](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313161832628.png)

       - 过程总结

         ![image-20200313161943566](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313161943566.png)

    4. Reconstruction from compressed representation

       ![image-20200313162229909](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313162229909.png)

    5. 应用

       - 将训练集的维度减少

         ![image-20200313162610352](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313162610352.png)

       - 不要使用PCA to prevent overfitting：因为会减少某些特征。use regularization instead。

         ![image-20200313162914805](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313162914805.png)

       - PCA只有在x集无效的情况下，才使用PCA去得到z集。

         ![image-20200313163055238](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313163055238.png)

15.  Anomaly detection

    - density estimation：给定一个正常数据集，然后一个test数据，检查test是否正常。

      ![image-20200313164623805](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313164623805.png)

      - application

        ![image-20200313164901247](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313164901247.png)

    1. Gaussian distribution

    2. 实现

       - 也就是在每个特征求Gaussian distribution，然后把每个特征乘起立，即可判断是不是异常的。

         ![image-20200313170423088](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313170423088.png)

    3. 检测异常检测是否正确

       - 通过带标签的数据集进行判断:

         ![image-20200313171501967](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313171501967.png)

       - 使用Precision/Recall等方法判断，因为数据集是一个倾斜的。然后要改变ε值看准确率怎么变化。

         ![image-20200313171845733](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313171845733.png)

    4. Anomaly detection vs. Supervised learning

       - 正负数据相差悬殊，有一些异常情况可能现在还未出现，使用Anomaly detection。正负数据差不多，且大部分情况都已经出现了，使用Supervised learning。

         ![image-20200313172512387](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313172512387.png)

    5.  

       - 如果数据Non-gaussian features：可以进行调整

         ![image-20200313174117710](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313174117710.png)

       - Error analysis for anomaly detection：去除异常特征，通过查看为什么这个特征有问题。

         ![image-20200313190514780](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313190514780.png)

       - 可以通过组合去组合一些特征作为新特征：

         ![image-20200313190653232](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313190653232.png)

    6. Multivariate Gaussian distribution：有时候更准

       ![image-20200313191046853](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313191046853.png)

       - 计算

         ![image-20200313193027148](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313193027148.png)

       - Multivariate Gaussian Model is a special Original model：只要Σ的两个三角形都是0，那么Multicariate Gaussian same as Ogrinal model.

         ![image-20200313193242761](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313193242761.png)

    7. Original model vs. Multicariate Gaussian

       - Original model 计算量少，需要的m少。Multivariate Gaussian计算量大，m大，但是效果好一点。

         ![image-20200313193830983](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200313193830983.png)

16. recommender systems

    1. 

