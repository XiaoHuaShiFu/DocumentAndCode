# 2、神经网络基础

1. 二分分类

   - Notation

     ![image-20200408204705412](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200408204705412.png)

2. logistic regression cost function

   ![image-20200408211022280](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200408211022280.png)

3. 梯度下降

   ![image-20200409005017750](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200409005017750.png)

4. 向量化

   1. ![image-20200409010332010](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200409010332010.png)
   2. ![image-20200409011446703](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200409011446703.png)
   3. ![image-20200409012352294](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200409012352294.png)
   4. ![image-20200409012844343](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200409012844343.png)

# 3、浅层神经网络

1. 逐层拆解

   ![image-20200410005401897](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200410005401897.png)

2. vectorizing across multiple examples

   ![image-20200410010558779](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200410010558779.png)

   

3. Activation function：激活函数均值接近0比较好

   - ![image-20200410011912098](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200410011912098.png)

   - tanh：均值为0，[-1, 1]
     - 几乎都比sigmoid函数好。除了在输出层外，因为如果希望输出是0~1，sigmoid可以把输出值调整到0~1，在二元分类的时候可以使用sigmoid作为输出层的激活函数。
   - **默认规则：除了二元分类用sigmoid作为输出层的激活函数外，都用Relu。**
   - 如果用线性激活函数，那么不管经过多少层，都还是输出线性函数，因此无法拟合复杂的情况。**除了在输出层需要把输出转换成线性的函数（-∞，+∞）。**

4. Summary of gradient descent：也是可以向量化的

   ![image-20200410015618019](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200410015618019.png)

5. random initialization：要乘一个很小的数，这样开始训练的时候才会快。因为sigmoid在值大的时候斜率很小。

   ![image-20200410020819588](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20200410020819588.png)